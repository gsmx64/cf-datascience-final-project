{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01f4724-f7de-488c-85d8-5917870eb6d7",
   "metadata": {},
   "source": [
    "# PROYECTO FINAL DE CIENCIA DE DATOS - BOOTCAMP CÓDIGO FACILITO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1651112",
   "metadata": {},
   "source": [
    "## Realizado por Gonzalo Mahserdjian y Simón Maquilón"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fef0f-927e-4ef5-8c4f-affdd6aed9d0",
   "metadata": {},
   "source": [
    "### INSTALAR REQUERIMIENTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9e98a",
   "metadata": {},
   "source": [
    "Instalo los requerimientos necesarios para este Jupiter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install python-dotenv polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0972a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8d7df-b476-4222-b9b9-b1ee9a8ac529",
   "metadata": {},
   "source": [
    "### OBTENCIÓN DE DATOS PARA DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801ad89",
   "metadata": {},
   "source": [
    "Para armar el DataSet con los datos con los que voy a trabajar, realicé una gran consulta de SQL (Big Query), como traer todos los años juntos a veces se colgaba la conexión por VPN o traía menos datos, los separé primero por años y luego los unifiqué en un único arhivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731afc5-0330-4298-abf1-3c5859aa2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pyodbc\n",
    "import asyncio\n",
    "import polars as pl\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", verbose=True)\n",
    "cfg = dotenv_values(\".env\")\n",
    "\n",
    "DATASET_SYNC = False\n",
    "PRINT_QUERY = False\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2023\n",
    "DATA_PATH = 'data/'\n",
    "SQL_FILE_PATH = 'sql/bigquery.sql'\n",
    "\n",
    "def read_sql_file(path: str, search: str, replace: str) -> None:\n",
    "    \"\"\"\n",
    "    Edits file content\n",
    "    \"\"\"\n",
    "    if Path(path).suffix == '.sql':\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                data = file.read()\n",
    "\n",
    "            return data.replace(search, replace)\n",
    "\n",
    "        except FileNotFoundError as error:\n",
    "            print(error)\n",
    "            return None\n",
    "\n",
    "def get_query(year: str) -> str:\n",
    "    return read_sql_file(SQL_FILE_PATH, '{year}', year)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    try:\n",
    "        db_string = f\"\"\"DRIVER={cfg['DB_DRIVER']};\n",
    "                        SERVER={cfg['DB_HOST']};\n",
    "                        DATABASE={cfg['DB_DATABASE']};\n",
    "                        UID={cfg['DB_USER']};\n",
    "                        PWD={cfg['DB_PASSWORD']}\"\"\"\n",
    "\n",
    "        if DATASET_SYNC:\n",
    "            connect = pyodbc.connect(db_string)\n",
    "\n",
    "            for year in range(START_YEAR, END_YEAR+1):\n",
    "                if(PRINT_QUERY):\n",
    "                    print(get_query(str(year)))\n",
    "\n",
    "                with connect.cursor() as cursor:\n",
    "                    cursor.execute(get_query(str(year)))\n",
    "                    rows = cursor.fetchall()\n",
    "\n",
    "                    with open(r'./'+DATA_PATH+'/DataSet_'+str(year)+'.csv', 'w', encoding=\"utf-8\", newline='') as csvfile:\n",
    "                        writer = csv.writer(csvfile)\n",
    "                        writer.writerow([x[0] for x in cursor.description])  # column headers\n",
    "                        \n",
    "                        for row in rows:\n",
    "                            writer.writerow(row)\n",
    "\n",
    "            await asyncio.sleep(10)\n",
    "\n",
    "            df = pl.read_csv(f'{DATA_PATH}DataSet_*.csv')\n",
    "            df.write_csv(f'{DATA_PATH}DataSet.csv')\n",
    "        \n",
    "            print('>>> ¡Dataset sincronizado con éxito!')\n",
    "        else:\n",
    "            print('>>> ¡Sincronización de Dataset no habilitada, se utilizará la versión offline!')\n",
    "\n",
    "    except pyodbc.OperationalError as err:\n",
    "        print('No fue posible realizar la conexión!')\n",
    "        print(err)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade75bc",
   "metadata": {},
   "source": [
    "### LECTURA DEL CSV Y DEPURACIÓN DEL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b270c3c",
   "metadata": {},
   "source": [
    "Variables para el DataSet unificado con los últimos 6 años y para el DataSet limpio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ae582",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{DATA_PATH}DataSet.csv'\n",
    "file_path_cleaned = f'{DATA_PATH}DataSetCleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b7de4",
   "metadata": {},
   "source": [
    "Leo el DataSet unificado, y visualizo las primeras 5 columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbfa69-a3ef-49c9-b7c7-58166a86dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284072a9",
   "metadata": {},
   "source": [
    "Exploro las las métricas básicas sobre las columnas del DataSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dd2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb9ef9",
   "metadata": {},
   "source": [
    "Dada la característica del DataSet que armé y a donde apunto con los mismos mucho no me sirven estos resultados, por empezar las primeras 2 columnas son IDs de clientes y sus contratos, tengo strings de nombres de barrios y constantes de nombres de planes, periodos, etc, lo único rescatable sería que \"1 de Mayo\" es el barrio que menor cantidad de veces sale y su opuesto \"Villa Eucaristica\" es el que más sale, y por otro lado que 6mbps es el plan más chico (en 2018) de internet (es un plan viejo, actualmente 50mbps es el menor) y 300mbps es el mayor, más adelante voy a comprobar todo esto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4ab5e",
   "metadata": {},
   "source": [
    "Verifico la cantidad de líneas y columnas del DataSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238fd6f",
   "metadata": {},
   "source": [
    "A continuación verifico si hay valores NA provenientes desde la consulta SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.lit('NA').str.contains(pl.col('Internet_Tipo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.lit('NA').str.contains(pl.col('Estado')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555455c6",
   "metadata": {},
   "source": [
    "Busco si hay valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.all().null_count()).to_dicts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69325b",
   "metadata": {},
   "source": [
    "Se encontró un único valor nulo en columna \"Barrio\", realizo la limpieza..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80323ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fill_null(value='Sin barrio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2fc1e",
   "metadata": {},
   "source": [
    "Busco duplicados en la columna de \"ID_Cliente\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.count(\"ID_Cliente\").over(df.columns) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61374b8c",
   "metadata": {},
   "source": [
    "Unifico los duplicados de \"ID_Cliente\" manteniendo los primeros resultados de nada caso, como la consulta SQL ordena los más recientes primero, quedarían los contratos vigentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48756d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.unique(keep='first', maintain_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529ecc1",
   "metadata": {},
   "source": [
    "Realizo la misma búsqueda de duplicados en la columna de \"ID_Contrato\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.count(\"ID_Contrato\").over(df.columns) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd0546",
   "metadata": {},
   "source": [
    "Unifico ahora los duplicados de \"ID Contrato\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d94289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.unique(keep='first', maintain_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae74a3",
   "metadata": {},
   "source": [
    "Con el DataSet limpio, guardo todo en el archivo \"DataSetCleaned.csv\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_csv(file_path_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a938bd",
   "metadata": {},
   "source": [
    "Leo el nuevo DataSet y vuelvo a verificar si quedó algún nulo y duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(file_path_cleaned)\n",
    "df.select(pl.all().null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.all().null_count()).to_dicts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.count(\"ID_Contrato\").over(df.columns) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cac871",
   "metadata": {},
   "source": [
    "Visualizo parcialmente el DataSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5aad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93c049",
   "metadata": {},
   "source": [
    "### ANÁLISIS DE LOS DATOS OBTENIDOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b531a19",
   "metadata": {},
   "source": [
    "Luego de realizada la consulta con la big query al motor de MSSQL, y posterior limpieza de los datos con Polars completando 1 solo dato nulo de un barrio inexistente en la celda \"Barrio\" del cliente en cuestión y eliminando los duplicados a favor del más reciente, se verifica que los datos son consistentes poorque todas las lineas (rows) tienen la misma cantidad de valores en las columnas (cols).\n",
    "\n",
    "¿Cúal es el por qué de los duplicados?: La base de datos es de un sistema donde no estaba pensado para evaluar los upgrades de los planes de internet, entonces logré obtener ese dato comparando tablas con contratos que tengan algún ticket con leyenda \"Upgrade\" (mejora en el plan del servicio de internet) en el descriptor del mismo, con un margen de 12 meses y esto en algun que otro cliente me trajo esos duplicados. Para generalizar, es porque en algunos sufrieron mudanzas, otros se arrepintieron del servicio o bien le ofrecieron pasarse a fibra óptica y luego por imposibilidades técnicas u otras que no puedo comentar, aún esa zona no contará con tendido de fibra óptica, dejando el ticket en \"Time out\" y devolviendo al cliente solamente con el servicio de Aire o Inalámbrico.\n",
    "\n",
    "Como el gran trabajo se realizó en la big query y luego la limpieza, no se detectan anomalías. Bueno, creo que si podría citar una anomalía que me encontre al realizar la big query, en donde habían clientes que no entraban en la categoría de \"Upgrade\" pero no eran ni \"Altas\" ni \"Bajas\" del servicio, entonces los bauticé como \"Fail Upgrade\", porque fueron casos donde imposibilidades técnicas hicieron que no se concrete el Upgrade pero que no se perdía el cliente, quedaba con el mismo plan ya existente y sin cambios, o sea tampoco se subscribió al servicio de TV por IP.\n",
    "\n",
    "A continuacíon se evaluarán los datos para identificar tendencias evidentes, y responder a las siguientes preguntas:\n",
    "* Altas del servicio de internet SIPA-AIRE vs SIPA-FIBRA en los últimos 6 años.\n",
    "* Bajas del servicio de internet SIPA-AIRE vs SIPA-FIBRA en los últimos 6 años.\n",
    "* Mudanzas del servicio de internet SIPA AIRE a SIPA-FIBRA.\n",
    "* Ver que barrio es el que tiene mayor demanda en el tiempo.\n",
    "* Ver que ancho de banda es el que tiene mayor demanda en el tiempo.\n",
    "* Ver que plan de internet es el que tiene mayor demanda en el tiempo.\n",
    "* Ver que cantidad de clientes tienen contratado IPTV en el tiempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
